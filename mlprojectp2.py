# -*- coding: utf-8 -*-
"""MLProjectP2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P5LXG1_ocEaDJXkd3X91MdmF0Ds9imii
"""

import numpy as np
import pandas as pd
import seaborn as sns
#Import dataset

df = pd.read_csv('clean_dataset.csv')

df

#Find number of missing datapoints

df2 = df.dropna();

df2

#Same size -- no missing values

#1-1
sns.displot(data=df2, x='Approved')

#Credit approvals-- roughly 60/40 not approved to approved.

#1-2 no na values--dataset is clean

#1-3 all columns usable, all kept

#1-4
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
ohe = preprocessing.OneHotEncoder()

df2['Industry'] = le.fit_transform(df2['Industry'])
df2['Ethnicity'] = le.fit_transform(df2['Ethnicity'])
df2['Citizen'] = le.fit_transform(df2['Citizen'])

df2

#1-5

df2['AvgAge'] = df2['Age'].mean()
df2['StdAge'] = df2['Age'].std()
df2['AvgInc'] = df2['Income'].mean()

df2

#1-6

#All columns were kept because they may hold some predictive value.
#Note--depending on the use of this model, it may be worth removing
#Gender/Ethnicity for ethical reasons.

#Industry, Ethnicity, and Citizen are all non-ordinal data and so
#they were label encoded. all other data is numeric.

#1-7

records = df2.filter(items={'Gender', 'Age', 'Debt', 'Married', 'BankCustomer', 'Industry', 'Ethnicity',
                            'YearsEmploye', 'PriorDefault', 'Employed', 'CreditScore', 'DriversLicense',
                            'Citizen', 'ZipCode', 'Income'}).to_records(index=False)
x_data = list(records)

y_data = list(df2.filter(items={'Approved'}).to_records(index=False))

print(x_data)
print(y_data)

for i in range(len(x_data)):
  x_data[i] = tuple(x_data[i])
for i in range(len(y_data)):
  y_data[i] = tuple(y_data[i])

#1-8
from sklearn.preprocessing import MinMaxScaler

# fit scaler on training data
norm = MinMaxScaler().fit(x_data)

# transform training data
x_data_norm = norm.transform(x_data)

# fit scaler on training data
norm = MinMaxScaler().fit(y_data)

# transform training data
y_data_norm = norm.transform(y_data)

#1-9

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    x_data_norm, y_data_norm, test_size=0.1, random_state=12)

#1-10

#2-1
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression

#2-2
reg = LinearRegression().fit(X_train, y_train)


clf = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)

#3-1, 3-2
regPerformance = reg.score(X_test, y_test)

print(regPerformance)

clfPerformance = clf.score(X_test, y_test)

print(clfPerformance)

#3-3
import matplotlib.pyplot as plt

regPerformances = []
clfPerformances = []
percentages = ['20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%']

for i in range(2, 10):
  X_train, X_test, y_train, y_test = train_test_split(
    x_data_norm, y_data_norm, test_size=0.1*i, random_state=12)
  reg = LinearRegression().fit(X_train, y_train)
  clf = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)
  regPerformance = reg.score(X_test, y_test)
  clfPerformance = clf.score(X_test, y_test)
  regPerformances.append(regPerformance)
  clfPerformances.append(clfPerformance)

fig, ax = plt.subplots()
sns.lineplot(percentages, regPerformances, ax=ax, label='linreg') # first dataset
sns.lineplot(percentages, clfPerformances, ax=ax, label='tree') # second dataset

#3-4
residuals = []
for i in range(len(X_test)):
  residuals.append(reg.predict(np.asarray(X_test[i]).reshape(1, -1)) - y_test[i])

plt.scatter(residuals, y_test)

plt.show()

#Residuals are clustered around 0, with a few outliers to
#the left and right. Part of the residuals are stuck to the left
#on the 1 side, and part of the residuals are stuck to the right
#on the 0 side. Notably, the residuals are larger than the max
#variance in the dataset.

#3-5

#Found a library called dtreeviz and spent
#a while but couldn't get it to work.

predictions = []

for i in range(len(X_test)):
  predictions.append(clf.predict(np.asarray(X_test[i]).reshape(1, -1)))

y_test2 = y_test.tolist()
y_test3 = []
for i in range(len(y_test2)):
  y_test3.append(y_test2[i][0])

differences = []
for i in range(len(y_test3)):
  differences.append(int(y_test3[i]-predictions[i]))
sns.displot(data=differences)

#Gives a visualization of the density of
#estimation error vs success.